{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHUROpEPHb7lVZtAIgEPfP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arv-ind-s/content-moderation-system/blob/main/notebooks/03_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– Content Moderation System - Model Training\n",
        "\n",
        "## Objective\n",
        "Fine-tune DistilBERT for multi-label toxic comment classification with class imbalance handling.\n",
        "\n",
        "## Model Selection: DistilBERT\n",
        "\n",
        "**Why DistilBERT?**\n",
        "- 40% smaller than BERT (66M vs 110M parameters)\n",
        "- 60% faster inference (critical for real-time API)\n",
        "- Maintains 97% of BERT's performance\n",
        "- Fits AWS Lambda deployment constraints\n",
        "- Pre-trained on English language understanding\n",
        "\n",
        "## Training Strategy\n",
        "\n",
        "### Multi-Label Classification\n",
        "- Each of 6 toxicity categories treated as independent binary classification\n",
        "- Use Binary Cross-Entropy (BCE) loss with logits\n",
        "- Sigmoid activation for each label (not softmax)\n",
        "\n",
        "### Handling Class Imbalance (8.8:1 ratio)\n",
        "- **Weighted loss**: Higher penalty for misclassifying toxic comments\n",
        "- **Focal loss** (optional): Focus learning on hard examples\n",
        "- **Metrics**: F1-score, Precision, Recall (NOT accuracy)\n",
        "\n",
        "### Training Configuration\n",
        "- **Batch size**: 16 (balanced for GPU memory and training speed)\n",
        "- **Learning rate**: 2e-5 (recommended for fine-tuning transformers)\n",
        "- **Epochs**: 3-4 (transformers need few epochs)\n",
        "- **Optimizer**: AdamW (weight decay for regularization)\n",
        "- **Warmup steps**: 500 (gradual learning rate increase)\n",
        "- **Max sequence length**: 256 tokens (handles 95% of comments)\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "Given severe imbalance, we'll track:\n",
        "1. **Per-category F1-scores** (harmonic mean of precision/recall)\n",
        "2. **Precision** (avoid false positives - don't over-flag)\n",
        "3. **Recall** (catch toxic content - don't miss real toxicity)\n",
        "4. **ROC-AUC** (threshold-independent performance)\n",
        "5. **Confusion matrix** per category\n",
        "\n",
        "**Target**: F1 > 0.75 for \"toxic\" category (balanced precision/recall)\n",
        "\n",
        "---\n",
        "\n",
        "**Author**: Aravind S  \n",
        "**Date**: December 7, 2025  \n",
        "**Model**: distilbert-base-uncased  \n",
        "**Framework**: PyTorch + Transformers  \n",
        "**GitHub**:https://github.com/Arv-ind-s/content-moderation-system/blob/main/README.md\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SY_Ab-jrFYn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup and Install Dependencies"
      ],
      "metadata": {
        "id": "fRp0sA46FqXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers torch datasets scikit-learn accelerate\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "# AdamW is now part of torch.optim or transformers.optimization\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"âœ… Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "clMTrnLpFUiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize Tokenizer and Configuration\n",
        "\n",
        "DistilBERT uses WordPiece tokenization with:\n",
        "- Max sequence length: 256 tokens (covers 95%+ of comments)\n",
        "- Padding/truncation to handle variable lengths\n",
        "- Special tokens: [CLS] at start, [SEP] at end"
      ],
      "metadata": {
        "id": "_DQUmZR6Gzu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Configuration\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 16\n",
        "NUM_LABELS = 6  # 6 toxicity categories\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "print(f\"âœ… Tokenizer loaded: {MODEL_NAME}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "9vOrDE8aG1cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test tokenization on a sample\n",
        "train_df = pd.read_csv('/content/train_processed.csv', engine='python', on_bad_lines='skip')\n",
        "sample_text = train_df.iloc[0]['comment_text']\n",
        "encoded = tokenizer.encode_plus(\n",
        "    sample_text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(\"Sample text:\", sample_text[:100])\n",
        "print(\"\\nTokenized output:\")\n",
        "print(f\"Input IDs shape: {encoded['input_ids'].shape}\")\n",
        "print(f\"Attention mask shape: {encoded['attention_mask'].shape}\")\n",
        "print(f\"First 10 tokens: {encoded['input_ids'][0][:10].tolist()}\")\n",
        "print(f\"Decoded: {tokenizer.decode(encoded['input_ids'][0][:10])}\")"
      ],
      "metadata": {
        "id": "FSFgqxs6G-76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create Custom Dataset Class\n",
        "\n",
        "PyTorch Dataset for efficient batch loading with tokenization."
      ],
      "metadata": {
        "id": "5cdVNygQIKJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToxicCommentsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for toxic comment classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.FloatTensor(label)\n",
        "        }\n",
        "\n",
        "print(\"âœ… Dataset class defined\")"
      ],
      "metadata": {
        "id": "9MG2C7fpIMOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load all processed datasets with error handling\n",
        "train_df = pd.read_csv('/content/train_processed.csv', engine='python', on_bad_lines='skip')\n",
        "test_df = pd.read_csv('/content/test_processed.csv', engine='python', on_bad_lines='skip')\n",
        "\n",
        "# Create validation set from train_df by splitting it\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"âœ… Data loaded and split:\")\n",
        "print(f\"Train: {len(train_df):,} samples\")\n",
        "print(f\"Val:   {len(val_df):,} samples\")\n",
        "print(f\"Test:  {len(test_df):,} samples\")\n",
        "\n",
        "# Prepare labels as numpy arrays\n",
        "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "train_labels = train_df[label_cols].values\n",
        "val_labels = val_df[label_cols].values\n",
        "test_labels = test_df[label_cols].values\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ToxicCommentsDataset(\n",
        "    texts=train_df['comment_text'].values,\n",
        "    labels=train_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "val_dataset = ToxicCommentsDataset(\n",
        "    texts=val_df['comment_text'].values,\n",
        "    labels=val_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "test_dataset = ToxicCommentsDataset(\n",
        "    texts=test_df['comment_text'].values,\n",
        "    labels=test_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Datasets created:\")\n",
        "print(f\"Train: {len(train_dataset):,} samples\")\n",
        "print(f\"Val:   {len(val_dataset):,} samples\")\n",
        "print(f\"Test:  {len(test_dataset):,} samples\")"
      ],
      "metadata": {
        "id": "ihTTIMxZI46f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"âœ… DataLoaders created:\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches:   {len(val_loader)}\")\n",
        "print(f\"Test batches:  {len(test_loader)}\")\n",
        "\n",
        "# Test loading a batch\n",
        "batch = next(iter(train_loader))\n",
        "print(\"\\nSample batch shapes:\")\n",
        "print(f\"Input IDs: {batch['input_ids'].shape}\")\n",
        "print(f\"Attention mask: {batch['attention_mask'].shape}\")\n",
        "print(f\"Labels: {batch['labels'].shape}\")"
      ],
      "metadata": {
        "id": "cG1xl3aVJHcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Calculate Class Weights for Imbalanced Data\n",
        "\n",
        "With 8.8:1 clean-to-toxic ratio, we weight the loss to penalize misclassifying toxic comments more."
      ],
      "metadata": {
        "id": "d3iiWOCUJLwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate positive class weights for each label\n",
        "# (train_df already loaded above, but if you run this cell separately, uncomment next line)\n",
        "# train_df = pd.read_csv('/content/train_processed.csv', engine='python', on_bad_lines='skip')\n",
        "\n",
        "class_weights = []\n",
        "print(\"Class weights for imbalanced labels:\\n\")\n",
        "print(f\"{'Label':<20} {'Positive':>10} {'Negative':>10} {'Weight':>10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for col in label_cols:\n",
        "    pos_count = train_df[col].sum()\n",
        "    neg_count = len(train_df) - pos_count\n",
        "    weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
        "    class_weights.append(weight)\n",
        "\n",
        "    print(f\"{col:<20} {pos_count:>10,} {neg_count:>10,} {weight:>10.2f}\")\n",
        "\n",
        "# Convert to tensor and move to GPU\n",
        "pos_weight = torch.FloatTensor(class_weights).to(device)\n",
        "print(f\"\\nâœ… Class weights moved to {device}\")\n",
        "print(f\"Weights: {pos_weight}\")"
      ],
      "metadata": {
        "id": "ByDLrV_4JMgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Initialize DistilBERT Model"
      ],
      "metadata": {
        "id": "tfqsLw_nJVnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained DistilBERT for multi-label classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"âœ… Model loaded and moved to {device}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "id": "TBtCc_VpJUeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Setup Optimizer and Learning Rate Scheduler\n",
        "\n",
        "- Optimizer: AdamW (Adam with weight decay for regularization)\n",
        "- Learning rate: 2e-5 (standard for fine-tuning transformers)\n",
        "- Scheduler: Linear warmup then decay"
      ],
      "metadata": {
        "id": "Ictb7GNlJkG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "# Calculate total training steps\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "\n",
        "# Setup learning rate scheduler with warmup\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=500,  # Warmup for first 500 steps\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"âœ… Optimizer and scheduler configured\")\n",
        "print(f\"Total training steps: {total_steps:,}\")\n",
        "print(f\"Warmup steps: 500\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")"
      ],
      "metadata": {
        "id": "O8SGgZbCJk0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Cross Entropy with Logits Loss (handles multi-label)\n",
        "# Uses pos_weight to handle class imbalance\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "print(f\"âœ… Loss function: BCEWithLogitsLoss with class weights\")\n",
        "print(f\"Class weights: {pos_weight}\")"
      ],
      "metadata": {
        "id": "67DO6ljDJrZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training and Validation Functions"
      ],
      "metadata": {
        "id": "N0K_VAoDJu34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress every 500 batches\n",
        "        if (batch_idx + 1) % 500 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(data_loader)} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation/test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions (apply sigmoid for probabilities)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()  # Threshold at 0.5\n",
        "\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "\n",
        "    return avg_loss, all_preds, all_labels\n",
        "\n",
        "print(\"âœ… Training and evaluation functions defined\")"
      ],
      "metadata": {
        "id": "3kty4chNJvnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Train the Model\n",
        "\n",
        "Training for 3 epochs with validation after each epoch."
      ],
      "metadata": {
        "id": "o83Fnjq-Jz3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_preds, val_labels = eval_model(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Calculate F1 scores per label\n",
        "    print(\"\\nValidation F1-Scores per category:\")\n",
        "    for idx, label in enumerate(label_cols):\n",
        "        f1 = f1_score(val_labels[:, idx], val_preds[:, idx], zero_division=0)\n",
        "        print(f\"  {label:<20}: {f1:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(f\"\\nâœ… Best model saved (Val Loss: {val_loss:.4f})\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Training Complete!\")"
      ],
      "metadata": {
        "id": "Dtcba0yHJ13S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive if not already mounted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "# Define the directory path\n",
        "save_dir = '/content/drive/MyDrive/content_moderation/models'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Copy best model to Drive\n",
        "!cp best_model.pt {save_dir}/best_model.pt\n",
        "\n",
        "# Also save with metadata\n",
        "import torch\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_name': MODEL_NAME,\n",
        "    'num_labels': NUM_LABELS,\n",
        "    'max_length': MAX_LENGTH,\n",
        "    'label_cols': label_cols,\n",
        "    'class_weights': class_weights\n",
        "}, f'{save_dir}/best_model_with_config.pt')\n",
        "\n",
        "print(\"âœ… Model saved to Google Drive\")"
      ],
      "metadata": {
        "id": "ljKY5f0ym7Xl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}